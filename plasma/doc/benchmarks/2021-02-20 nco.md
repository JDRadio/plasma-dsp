# Baseline
## Description
Baseline measurements.

## Results
```
2021-02-20T15:50:13-05:00
Running bin/test
Run on (16 X 3700 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x8)
  L1 Instruction 64 KiB (x8)
  L2 Unified 512 KiB (x8)
  L3 Unified 8192 KiB (x2)
Load Average: 0.58, 0.76, 0.62
***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.
-----------------------------------------------------------------
Benchmark                       Time             CPU   Iterations
-----------------------------------------------------------------
nco_up<float>/1000       36760608 ns     36754606 ns           19
nco_down<float>/1000     36335051 ns     36329238 ns           19
nco_up<double>/1000      40918294 ns     40911303 ns           17
nco_down<double>/1000    40554552 ns     40548602 ns           17
nco_up<float>/10000    3705752045 ns   3705132287 ns            1
nco_down<float>/10000  3641660915 ns   3641008778 ns            1
nco_up<double>/10000   4083124667 ns   4082393763 ns            1
nco_down<double>/10000 4077645676 ns   4076959875 ns            1
```

## Analysis
Both up and down branches take the same amount of time.


# `std::tie` is slow
## Description
Replacing calls to `std::tie` with `auto [a, b]` seems to speed things up. Only done on `nco_up` branch for comparison.

## Results
```
2021-02-20T15:51:30-05:00
Running bin/test
Run on (16 X 3700 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x8)
  L1 Instruction 64 KiB (x8)
  L2 Unified 512 KiB (x8)
  L3 Unified 8192 KiB (x2)
Load Average: 0.35, 0.66, 0.60
***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.
-----------------------------------------------------------------
Benchmark                       Time             CPU   Iterations
-----------------------------------------------------------------
nco_up<float>/1000       29990030 ns     29983802 ns           23
nco_down<float>/1000     36678948 ns     36671469 ns           19
nco_up<double>/1000      35033708 ns     35026550 ns           20
nco_down<double>/1000    41339442 ns     41331861 ns           17
nco_up<float>/10000    2988461850 ns   2987956378 ns            1
nco_down<float>/10000  3691230944 ns   3690547354 ns            1
nco_up<double>/10000   3569590516 ns   3568759320 ns            1
nco_down<double>/10000 4177131662 ns   4176332187 ns            1
```

## Analysis
These results confirm the call to `std::tie` is indeed slow. Good gain in the `nco_up` branch compared to `nco_down`.


# Modify by reference
## Description
Instead of returning a `std::pair` of values for a single sample, the modification is done in place. Again, only on `nco_up` branch.

## Results
```
2021-02-20T15:56:58-05:00
Running bin/test
Run on (16 X 3700 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x8)
  L1 Instruction 64 KiB (x8)
  L2 Unified 512 KiB (x8)
  L3 Unified 8192 KiB (x2)
Load Average: 0.55, 0.56, 0.56
***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.
-----------------------------------------------------------------
Benchmark                       Time             CPU   Iterations
-----------------------------------------------------------------
nco_up<float>/1000       14409392 ns     14406829 ns           49
nco_down<float>/1000     29988749 ns     29982935 ns           23
nco_up<double>/1000      18248594 ns     18245002 ns           38
nco_down<double>/1000    34805755 ns     34799750 ns           20
nco_up<float>/10000    1440743291 ns   1440433065 ns            1
nco_down<float>/10000  2995302726 ns   2994730105 ns            1
nco_up<double>/10000   1821769756 ns   1821400193 ns            1
nco_down<double>/10000 3492138293 ns   3491424281 ns            1
```

## Analysis
Major improvement by using references and modifying in place.


# Save a function call
## Description
Trying to save a function call, let's move the sample modification code inside.

## Results
```
2021-02-20T16:01:08-05:00
Running bin/test
Run on (16 X 3700 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x8)
  L1 Instruction 64 KiB (x8)
  L2 Unified 512 KiB (x8)
  L3 Unified 8192 KiB (x2)
Load Average: 0.25, 0.44, 0.51
***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.
-----------------------------------------------------------------
Benchmark                       Time             CPU   Iterations
-----------------------------------------------------------------
nco_up<float>/1000       14944969 ns     14942230 ns           47
nco_down<float>/1000     29702997 ns     29698024 ns           24
nco_up<double>/1000      19631893 ns     19628136 ns           36
nco_down<double>/1000    34646132 ns     34640103 ns           20
nco_up<float>/10000    1472647038 ns   1472356778 ns            1
nco_down<float>/10000  2977419355 ns   2976816437 ns            1
nco_up<double>/10000   1876718697 ns   1876337673 ns            1
nco_down<double>/10000 3537552334 ns   3536773956 ns            1
```

## Analysis
No measurable outcome. The compiler was probably already optimizing this part. Let's revert back.


# New baseline
## Description
Establishing a new baseline by reciprocating the changes onto `nco_down`.

## Results
```
2021-02-20T16:10:44-05:00
Running bin/test
Run on (16 X 3700 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x8)
  L1 Instruction 64 KiB (x8)
  L2 Unified 512 KiB (x8)
  L3 Unified 8192 KiB (x2)
Load Average: 0.34, 0.49, 0.53
***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.
-----------------------------------------------------------------
Benchmark                       Time             CPU   Iterations
-----------------------------------------------------------------
nco_up<float>/1000       14221749 ns     14219394 ns           50
nco_down<float>/1000     14202365 ns     14200252 ns           49
nco_up<double>/1000      18340081 ns     18337164 ns           38
nco_down<double>/1000    18277872 ns     18274504 ns           38
nco_up<float>/10000    1423829112 ns   1423571574 ns            1
nco_down<float>/10000  1422293067 ns   1422026782 ns            1
nco_up<double>/10000   1836432655 ns   1836126897 ns            1
nco_down<double>/10000 1812677782 ns   1812392169 ns            1
```

## Analysis
The small changes we've made had a considerable impact on the results.
```
--------------------------------------------------------------------------------
Benchmark                     Baseline     New Baseline   Ratio            Delta
--------------------------------------------------------------------------------
nco_up<float>/1000        36.754606 ms     14.219394 ms   38.7%     22.535212 ms
nco_down<float>/1000      36.329238 ms     14.200252 ms   39.1%     22.128986 ms
nco_up<double>/1000       40.911303 ms     18.337164 ms   44.8%     22.574139 ms
nco_down<double>/1000     40.548602 ms     18.274504 ms   45.1%     22.274098 ms
nco_up<float>/10000     3705.132287 ms   1423.571574 ms   38.4%   2281.560713 ms
nco_down<float>/10000   3641.008778 ms   1422.026782 ms   39.1%   2218.981996 ms
nco_up<double>/10000    4082.393763 ms   1836.126897 ms   45.0%   2246.266866 ms
nco_down<double>/10000  4076.959875 ms   1812.392169 ms   44.5%   2264.567706 ms
```


# Fixing the testÂ bench
## Description
The timing values are very poor. It takes 14 ms to process 1000 samples?! Something is wrong.

It turns out it was an issue with the test bench. Now that it's fixed, let's get a real baseline.
Our improvements are not lost however, those gains were real.

First, the number of samples was wrong. It was 1000 elements, not 1000 samples, so that has been corrected.
Second, the test was performing the operation N times instead of just a single time per iteration.

## Results
```
2021-02-20T16:33:06-05:00
Running bin/test
Run on (16 X 3700 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x8)
  L1 Instruction 64 KiB (x8)
  L2 Unified 512 KiB (x8)
  L3 Unified 8192 KiB (x2)
Load Average: 0.71, 0.53, 0.48
***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.
---------------------------------------------------------------------
Benchmark                           Time             CPU   Iterations
---------------------------------------------------------------------
nco_up<float>/1000              28491 ns        28486 ns        24507
nco_down<float>/1000            28440 ns        28434 ns        24605
nco_up<double>/1000             37242 ns        37232 ns        18761
nco_down<double>/1000           37148 ns        37139 ns        19289
nco_up<float>/10000            306078 ns       306004 ns         2320
nco_down<float>/10000          287588 ns       287530 ns         2457
nco_up<double>/10000           367078 ns       366971 ns         1860
nco_down<double>/10000         365042 ns       364968 ns         1838
nco_up<float>/100000          2868822 ns      2868020 ns          245
nco_down<float>/100000        2879419 ns      2878650 ns          242
nco_up<double>/100000         3657842 ns      3656839 ns          191
nco_down<double>/100000       3754991 ns      3753820 ns          193
nco_up<float>/10000000      287600339 ns    287525910 ns            2
nco_down<float>/10000000    286512155 ns    286449584 ns            2
nco_up<double>/10000000     366505910 ns    366378921 ns            2
nco_down<double>/10000000   364963261 ns    364840563 ns            2
nco_up<float>/100000000    2860285712 ns   2859603344 ns            1
nco_down<float>/100000000  2861424042 ns   2860729615 ns            1
nco_up<double>/100000000   3683732403 ns   3682486029 ns            1
nco_down<double>/100000000 3675849235 ns   3674636274 ns            1
```

## Analysis
Those number are much more encouraging and far more realistic. I have added 100k samples, 10M samples, and 100M samples.

Considering a radio sample rate of 10 Msps, it would take 283 ms to process 1000 ms of samples if they are `float`s, and 369 ms if they are `double`s.

With a radio sample rate of 100 Msps (most SDRs go up to 50 Msps only), it would take around 3 seconds to process a single second.
This is however not a realistic use case, as the signal should at least be downsampled a bit before frequency translation.

Looking at the 1k, 10k, 100k, 10M, and 100M timings, they are very consistent in that the 100k timing is roughly 100 times the 1k timing, and the 100M timing is 10 times the 10M timing.
